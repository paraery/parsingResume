{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paraery/parsingResume/blob/main/transformersModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opyhpofrsKIj",
        "outputId": "7341515b-34b4-43a8-e5cb-43bc84fe65e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 38.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LsXzUFoys03g"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "19d1906fc7624699b980554b35be7464",
            "c020071645bc443abd08af0b0cf2aa5e",
            "375d8d50198142038c4fa3df34003202",
            "8d392f972f0b40ad8ddf481f26728a13",
            "4a74b5a02dad42eabb3a31540c2d4899",
            "5f39155ca198420087051d19b129dc09",
            "5b52eb97a35a45eeaaab0651282788b0",
            "fd66afd5136b40129d7944bac181384e",
            "8e4170729df64dde8146e723c28a517d",
            "7e7e6c14ca96437f96c9b77f390a65fb",
            "8b2bfa3a3f024b8e9a18a77cc6845bff"
          ]
        },
        "id": "HHwMJPjet926",
        "outputId": "ff6e4410-3c80-4b58-b81f-4203b6b2455c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19d1906fc7624699b980554b35be7464"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertForTokenClassification: ['pre_classifier.weight', 'pre_classifier.bias']\n",
            "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Is71Ng-yVx-U"
      },
      "outputs": [],
      "source": [
        "NER = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "example = \"My name is yugesh and I live in India\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a12nsAtsXQjP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('drive/MyDrive/textfortraining3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LMha8PzBXYLK"
      },
      "outputs": [],
      "source": [
        "lstTxt = df['text'].values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-ameWiUxR9V"
      },
      "source": [
        "#train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4s23x8QzxQA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441de1ad-113d-4157-f08a-d62501789dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 451 kB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 51.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 64.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 39.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cHqMNeCixVMp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def get_tokens_with_entities(raw_text: str):\n",
        "    # split the text by spaces only if the space does not occur between square brackets\n",
        "    # we do not want to split \"multi-word\" entity value yet\n",
        "    raw_tokens = re.split(r\"\\s(?![^\\[]*\\])\", raw_text)\n",
        "\n",
        "    # a regex for matching the annotation according to our notation [entity_value](entity_name)\n",
        "    entity_value_pattern = r\"\\[(?P<value>.+?)\\]\\((?P<entity>.+?)\\)\"\n",
        "    entity_value_pattern_compiled = re.compile(entity_value_pattern, flags=re.I|re.M)\n",
        "\n",
        "    tokens_with_entities = []\n",
        "\n",
        "    for raw_token in raw_tokens:\n",
        "        match = entity_value_pattern_compiled.match(raw_token)\n",
        "        if match:\n",
        "            raw_entity_name, raw_entity_value = match.group(\"entity\"), match.group(\"value\")\n",
        "\n",
        "            # we prefix the name of entity differently\n",
        "            # B- indicates beginning of an entity\n",
        "            # I- indicates the token is not a new entity itself but rather a part of existing one\n",
        "            for i, raw_entity_token in enumerate(re.split(\"\\s\", raw_entity_value)):\n",
        "                entity_prefix = \"B\" if i == 0 else \"I\"\n",
        "                entity_name = f\"{entity_prefix}-{raw_entity_name}\"\n",
        "                tokens_with_entities.append((raw_entity_token, entity_name))\n",
        "        else:\n",
        "            tokens_with_entities.append((raw_token, \"O\"))\n",
        "\n",
        "    return tokens_with_entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yEhe_F97zwQw"
      },
      "outputs": [],
      "source": [
        "class NERDataMaker:\n",
        "    def __init__(self, texts):\n",
        "        self.unique_entities = []\n",
        "        self.processed_texts = []\n",
        "\n",
        "        temp_processed_texts = []\n",
        "        for text in texts:\n",
        "            tokens_with_entities = get_tokens_with_entities(text)\n",
        "            for _, ent in tokens_with_entities:\n",
        "                if ent not in self.unique_entities:\n",
        "                    self.unique_entities.append(ent)\n",
        "            temp_processed_texts.append(tokens_with_entities)\n",
        "\n",
        "        self.unique_entities.sort(key=lambda ent: ent if ent != \"O\" else \"\")\n",
        "\n",
        "        for tokens_with_entities in temp_processed_texts:\n",
        "            self.processed_texts.append([(t, self.unique_entities.index(ent)) for t, ent in tokens_with_entities])\n",
        "\n",
        "    @property\n",
        "    def id2label(self):\n",
        "        return dict(enumerate(self.unique_entities))\n",
        "\n",
        "    @property\n",
        "    def label2id(self):\n",
        "        return {v:k for k, v in self.id2label.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        def _process_tokens_for_one_text(id, tokens_with_encoded_entities):\n",
        "            ner_tags = []\n",
        "            tokens = []\n",
        "            for t, ent in tokens_with_encoded_entities:\n",
        "                ner_tags.append(ent)\n",
        "                tokens.append(t)\n",
        "\n",
        "            return {\n",
        "                \"id\": id,\n",
        "                \"ner_tags\": ner_tags,\n",
        "                \"tokens\": tokens\n",
        "            }\n",
        "\n",
        "        tokens_with_encoded_entities = self.processed_texts[idx]\n",
        "        if isinstance(idx, int):\n",
        "            return _process_tokens_for_one_text(idx, tokens_with_encoded_entities)\n",
        "        else:\n",
        "            return [_process_tokens_for_one_text(i+idx.start, tee) for i, tee in enumerate(tokens_with_encoded_entities)]\n",
        "\n",
        "    def as_hf_dataset(self, tokenizer):\n",
        "        from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
        "        def tokenize_and_align_labels(examples):\n",
        "            tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "            labels = []\n",
        "            for i, label in enumerate(examples[f\"ner_tags\"]):\n",
        "                word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "                previous_word_idx = None\n",
        "                label_ids = []\n",
        "                for word_idx in word_ids:  # Set the special tokens to -100.\n",
        "                    if word_idx is None:\n",
        "                        label_ids.append(-100)\n",
        "                    elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                        label_ids.append(label[word_idx])\n",
        "                    else:\n",
        "                        label_ids.append(-100)\n",
        "                    previous_word_idx = word_idx\n",
        "                labels.append(label_ids)\n",
        "\n",
        "            tokenized_inputs[\"labels\"] = labels\n",
        "            return tokenized_inputs\n",
        "\n",
        "        ids, ner_tags, tokens = [], [], []\n",
        "        for i, pt in enumerate(self.processed_texts):\n",
        "            ids.append(i)\n",
        "            pt_tokens,pt_tags = list(zip(*pt))\n",
        "            ner_tags.append(pt_tags)\n",
        "            tokens.append(pt_tokens)\n",
        "        data = {\n",
        "            \"id\": ids,\n",
        "            \"ner_tags\": ner_tags,\n",
        "            \"tokens\": tokens\n",
        "        }\n",
        "        features = Features({\n",
        "            \"tokens\": Sequence(Value(\"string\")),\n",
        "            \"ner_tags\": Sequence(ClassLabel(names=dm.unique_entities)),\n",
        "            \"id\": Value(\"int32\")\n",
        "        })\n",
        "        ds = Dataset.from_dict(data, features)\n",
        "        tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
        "        return tokenized_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPgq9ObOzdpo",
        "outputId": "259c981d-2274-412d-8ccb-1a36f6054234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total examples = 95\n",
            "[{'id': 0, 'ner_tags': [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['Thanyarat', 'Iemtadanai', '85/49', 'vista', 'vile', 'village,', 'lumlukka,', 'lumlukka,', 'pathum', 'thani,', 'thailand', '12150', '', 'pupaekamolwan@hotmail.com', '0814080375', '', '', '', 'education', '', 'chulalongkorn', 'university', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'bangkok,', 'thailand', 'bachelor', 'of', 'electrical', 'engineering', '', '', '', '', '', '', '', '', '', '', '', '', '2014-2018', '\\x95', 'gpax:', '3.55', '', '', 'triamudomsuksa', 'pattanakarn', 'lumlukka', 'school', '', '', '', '', '', '', '', '', '', '', '', '', 'pathum', 'thani,', 'thailand', 'science-mathematic', 'major', '', '', '', '', '', '2010-2013', '\\x95', 'gpax:', '3.90', '', '', 'activity', '', '\\x95', 'join', 'the', 'competition', 'tesco', 'lotus', 'hackathon', 'in', 'theme', 'retailer', '4.0:', 'think', 'innovation', 'to', 'develop', '', '', '', '', '', '', '', '', '', '', '', '2018', 'retailor', 'with', 'other', 'in', 'team', '\\x95', 'visit', 'power', 'plant', 'of', 'egco', 'at', 'rayong,', 'thailand', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '2016', '\\x95', 'attend', '2thegat', 'engineer', 'camp', 'at', 'chonburi,', 'thailand:visit', 'bangpakong', 'power', 'plant', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '2016', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '\\x95', 'a', 'staff', 'of', 'mega', 'trend', 'department', 'in', 'the', '17th', 'nitad', 'which', 'held', 'in', '15th', '\\x96', '19th', 'march', '2017', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '2016', 'in', 'field', 'of', 'robotic', ':', 'look', 'for', 'project', 'that', 'suitable', 'for', 'robotic', 'trend,', 'operate', 'and', 'design', 'presentation.', '', '', '', '\\x95', 'a', 'staff', 'of', 'activity', 'department', 'in', 'the', 'mahagam', 'tumdee', 'project', 'which', 'activities', 'are', 'paint', 'crosswalk', '', '', '', '', '', '', '', '2014', 'and', 'clean', 'footpath', 'at', 'chulalongkorn', 'university:', 'plan', 'and', 'prepare', 'tools', 'for', 'activity', '', '\\x95', 'a', 'staff', 'of', 'public', 'relation', 'in', 'make', 'good', 'thing', 'for', 'chula', 'project', 'which', 'activity', 'is', 'making', 'dolls', '', '', '', '', '', '', '', '', '', '', '', '', '2014', '', 'from', 'socks', 'for', 'children', ':', 'create', 'form', 'for', 'applying,', 'design', 'poster', 'and', 'leaflet', '', '', '', '', '', 'experience', 'delta', 'electronics', '(thailand)', 'pcl.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'samut', 'prakan,', 'thailand', 'embedded', 'firmware', 'engineer,', 'cnbu', 'department', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '2018-present', '\\x95', 'design', 'and', 'coding', 'firmware', 'for', 'power', 'supply', 'product', '', 'senior', 'project', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '2018', '\\x95', 'water', 'heater', 'control', 'system', 'with', 'mobile', 'application', '(software', 'part).', 'it', 'is', 'about', 'developing', 'normal', 'water', 'heater', 'by', 'creating', 'control', 'circuit', 'for', 'more', 'energy', 'saving', 'and', 'monitor', 'energy', 'consumption', 'in', 'mobile', 'application.', '', '', '', 'meinhardt(thailand)', 'ltd.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'bangkok,', 'thailand', 'summer', 'intern,', 'electrical', 'engineer,', 'm&e', 'department', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'summer', '2017', '\\x95', 'learn', 'about', 'electrical', 'system', 'design', 'in', 'building', '\\x95', 'design', 'fire', 'alarm', 'system', 'and', 'lightning', 'protection', 'system', 'in', 'building', '\\x95', 'learn', 'about', 'lighting', 'design', 'and', 'use', 'dialux', 'program', 'to', 'design', 'lighting', 'system', 'in', 'room', '', '', '', 'additional', 'information', '\\x95', 'languages:', 'english', '(toeic:', '675)', ',', 'thai(native)', '\\x95', 'computer', 'skills:', 'basic', 'java', 'programming,', 'python,', 'basic', 'matlab,', 'android', 'studio,', 'microchip', 'programming', 'and', 'sql', '\\x95', 'interests:', 'r&d,', 'electronic', 'circuit', 'design', '----------------page', '(0)', 'break----------------', '']}, {'id': 1, 'ner_tags': [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['Keerati', 'Plainukool', '', 'co', 'nta', 'ct', '511', 'samakee', 'rd.', 'thasai', 'district,', ',', 'nonthaburi,', '11000,', 'thailand', '+66936966299', 'autsadang41@gmail.com', '', 'link', 's', 'github:github.com/autsadang41', 'linkedin:autsadang', 'somboonphol', '', 'sk', 'ills', '', 'python:', 'beginner', '', 'c++:', 'beginner', '', '', 'latex:', 'beginner', '', '', 'microsoft', 'office:', 'skillful', '', 'teamwork:', 'skillful', '', '', 'leadership:', 'beginner', '', '', 'problem', 'solving:', 'beginner', '', '', 'communication:', 'skillful', '', '.', '', 'lan', 'gu', 'ag', 'es', '', 'thai:', '', '', '', '', '', 'native', '', '', 'english:', '', '', 'good', 'working', '', '', 'profile', '', 'i', 'am', 'a', 'researcher,', 'philosopher', 'and', 'entire', 'life', 'student.', 'i', 'was', 'a', 'politician.', 'you', 'will', 'notice', 'something', 'not', 'make', 'sense', 'or', 'consistent.', 'but', \"it's\", 'me,', 'and', 'the', 'world', 'too.', 'i', 'enjoy', 'exploring', 'the', 'world', 'that', 'inconsistent.', 'trust', 'me,', 'in', 'ultimately,', 'all', 'of', 'the', 'things', 'are', 'make', 'sense', 'in', 'the', 'end.', '', 'employment', 'history', '', 'former', 'co-founder,', 'former', 'executive', 'committee', 'member', 'at', 'former', 'glang', 'party(fair', 'party', 'current),', 'bangkok', ',', 'thailand', 'june', '2018', '\\x97', 'january', '2019', '?', 'leading', 'the', 'team', 'to', 'an', 'arrangement', 'and', 'directing,', 'coordinate,', 'solve', 'specific', 'problems', 'at', 'the', 'party', 'founder', 'meeting', 'event', '?', 'drive', 'personal', 'untitled', 'campaign', 'as', 'well', 'as', '\"to', 'close', 'senator', 'voter', 'switch\"', 'during', 'thailand', 'election', '?', 'summerize', 'and', 'indicate', 'with', 'taking', 'the', 'minutes', 'of', 'the', 'meeting,', 'comment', 'and', 'voting', 'at', 'the', 'party', 'decision', 'meeting', '?', 'draft', 'in', 'party', 'regulations,', 'arrangement', 'to', 'member', 'register', 'and', 'supervise', 'in', 'law', 'field', 'even', 'never', 'done', 'before', '?', 'done', 'in', 'the', 'football', 'competition', 'of', 'thailand', 'political', 'party', 'on', '15', 'december', '2018', 'with', 'communication', 'for', 'finding', 'a', 'place,', 'accommodation', 'and', 'coordination.', '?', 'facilitate', 'in', 'co-working', 'with', 'election', 'commission', 'of', 'thailand', 'as', 'coordinator', '', 'education', '', 'bachelor', 'of', 'political', 'science', 'in', 'public', 'administration,', 'ramkhamhaeng', 'university,', 'bangkok', ',', 'thailand,', 'gpa', '3.02', 'july', '2017', '\\x97', 'march', '2020', '', 'high', 'school,', 'benjamarachanusorn', 'school,', 'nonthaburi,', 'thailand,', 'gpa', '3.05', 'may', '2013', '\\x97', 'march', '2016', '', 'courses', '', '-programming', 'for', 'everybody', '(getting', 'started', 'with', 'python),', 'university', 'of', 'michigan', '', '-digital', 'skill:', 'artificial', 'intelligence,', 'futurelearn', '', '-apply', 'creative', 'machine', 'learning,', 'ual', 'creative', 'computing', 'institute', 'and', 'institute', 'of', 'coding', '', '-applications', 'of', 'ai', 'technology,', 'taipei', 'medical', 'university', '', '-foundation', 'of', 'neuroscience,', 'mahidol', 'university', '', '-introduction', 'to', 'data', 'analytics,', 'mahidol', 'university', '', '-fundamental', 'of', 'data', 'science,', 'rajamangala', 'university', 'of', 'technology', '', '-the', 'digital', 'work:', 'digital', 'marketing', 'analytics', '&', 'optimization,', 'thailand', 'e-business', 'center', 'co,.', 'ltd.', '', '-kon', 'gen', 'kla,', 'kla', 'party', '', '', '----------------page', '(0)', 'break----------------', '']}, {'id': 2, 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': [\"bachelor's\", 'degree', 'in', 'industrial', 'engineering', '[gpa: 3.77]', 'burn-in', 'test', 'of', 'knacksat', 'satellite', 'ground', 'station', 'operation', 'in', 'birds-2', 'satellite', 'project', 'the', '2nd', 'knacksat', 'ground', 'station', 'operation', 'workshop', 'organized', 'by', 'knacksat', 'satellite', 'project', 'teacher', 'assistance', '[spaceflight mechanics, metrology engineering, computer programming]', 'the', '2nd', 'national', 'metrology', 'skill', 'competition', 'thesis', 'on', 'development', 'of', 'navigation', 'system', 'and', 'graphical', 'user', 'interface', 'of', 'material', 'handling', 'robots', 'using', 'simultaneous', 'localization', 'and', 'mapping', 'as', 'a', 'backend', 'programmer', 'vocational', 'certificate', 'in', 'mechanics', '[gpa: 2.97]', 'worldskills', 'thailand', '2018', '[2nd runner up in dimensional metrology of samutprakarn institute for skill development]', 'king', \"mongkut's\", 'university', 'of', 'technology', 'north', 'bangkok', 'industrial', 'engineering', '|', '2017', 'to', '2021', '', '', '', '(university', 'level)', '[winner in dimensional metrology]', 'thai-german', 'pre-engineering', 'school', 'voc.', 'cert.', '(mechanics)', '|', '2014', 'to', '2017', 'academic', 'background', 'contact', 'info', 'phone:', '(+66)90-103-8202', 'email:', 's6001091620058@email.kmutnb.ac.th', 'Saowatharn', 'Jetjirawat', 'industrial', 'engineering', 'student', 'an', 'industrial', 'engineering', 'student', 'from', 'kmutnb', 'with', 'experiences', 'in', 'the', 'development', 'of', 'navigation', 'system', 'of', 'slam', 'robot,', 'dimensional', 'metrology', 'skill', 'competition,', 'and', 'ground', 'station', 'control', 'for', 'knacksat', 'personal', 'profile', 'computer', 'programing', 'dimensional', 'metrology', 'simulation', 'software', '(promodel)', 'statistical', 'analysis', 'software', '(minitab)', '', '', '', '(python,sql,c,c++,c#,matlab)', 'additional', 'skills', '----------------page', '(0)', 'break----------------', '']}]\n"
          ]
        }
      ],
      "source": [
        "dm = NERDataMaker(lstTxt)\n",
        "print(f\"total examples = {len(dm)}\")\n",
        "print(dm[0:3])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dm[11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsqbJ0a47oum",
        "outputId": "f69033f1-8e16-42b0-b054-74d5eca9db22"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 11,\n",
              " 'ner_tags': [0,\n",
              "  1,\n",
              "  2,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'tokens': ['miss',\n",
              "  'Malatree',\n",
              "  'Yoonim',\n",
              "  'address:',\n",
              "  '139/64',\n",
              "  'soi',\n",
              "  'khubon',\n",
              "  '27',\n",
              "  'yak',\n",
              "  '25,',\n",
              "  'tharaeng,',\n",
              "  'bangkhen,',\n",
              "  'bangkok,',\n",
              "  '10220,',\n",
              "  '',\n",
              "  'thailand',\n",
              "  'mobile',\n",
              "  'number:',\n",
              "  '+66',\n",
              "  '(0)',\n",
              "  '97',\n",
              "  '241',\n",
              "  '2151',\n",
              "  'e-mail:',\n",
              "  'nuttamol.j@gmail.com',\n",
              "  '',\n",
              "  'profile',\n",
              "  '\\x95',\n",
              "  'excellent',\n",
              "  'communication',\n",
              "  'skills',\n",
              "  'through',\n",
              "  'project',\n",
              "  'coordination',\n",
              "  'in',\n",
              "  'both',\n",
              "  'normal',\n",
              "  'and',\n",
              "  'urgent',\n",
              "  'situations,',\n",
              "  'adapt',\n",
              "  '',\n",
              "  'quickly',\n",
              "  'through',\n",
              "  'a',\n",
              "  'variety',\n",
              "  'of',\n",
              "  'tasks,',\n",
              "  'be',\n",
              "  'able',\n",
              "  'to',\n",
              "  'learn',\n",
              "  'and',\n",
              "  'think',\n",
              "  'critically',\n",
              "  'quickly,',\n",
              "  'and',\n",
              "  'be',\n",
              "  'creative',\n",
              "  'in',\n",
              "  'solving',\n",
              "  'problems',\n",
              "  'in',\n",
              "  'new',\n",
              "  'ways',\n",
              "  'by',\n",
              "  'meeting',\n",
              "  'different',\n",
              "  'types',\n",
              "  'of',\n",
              "  'people.',\n",
              "  '\\x95',\n",
              "  'intermediate',\n",
              "  'working',\n",
              "  'knowledge',\n",
              "  'of',\n",
              "  'microsoft',\n",
              "  'office,',\n",
              "  'google',\n",
              "  'drive,',\n",
              "  'and',\n",
              "  'being',\n",
              "  'able',\n",
              "  'to',\n",
              "  'use',\n",
              "  'various',\n",
              "  'tools',\n",
              "  'in',\n",
              "  'the',\n",
              "  'presentation.',\n",
              "  '\\x95',\n",
              "  'fluent',\n",
              "  'in',\n",
              "  'thai',\n",
              "  'and',\n",
              "  'english',\n",
              "  '(read',\n",
              "  'and',\n",
              "  'understand),',\n",
              "  'good',\n",
              "  'command',\n",
              "  'of',\n",
              "  'speak,',\n",
              "  'listen,',\n",
              "  'and',\n",
              "  'write',\n",
              "  'english.',\n",
              "  '',\n",
              "  'education',\n",
              "  'jul',\n",
              "  '2017',\n",
              "  '\\x96',\n",
              "  'apr',\n",
              "  '2021',\n",
              "  'bachelor',\n",
              "  'of',\n",
              "  'science',\n",
              "  'in',\n",
              "  'geography',\n",
              "  'with',\n",
              "  'first',\n",
              "  'class',\n",
              "  'honours',\n",
              "  '',\n",
              "  'faculty',\n",
              "  'of',\n",
              "  'social',\n",
              "  'sciences,',\n",
              "  'kasetsart',\n",
              "  'university,',\n",
              "  'thailand',\n",
              "  '',\n",
              "  'major:',\n",
              "  'geography',\n",
              "  'minor:',\n",
              "  'psychology',\n",
              "  'may',\n",
              "  '2014',\n",
              "  '\\x96',\n",
              "  'mar',\n",
              "  '2017',\n",
              "  'satriwitthaya',\n",
              "  '2',\n",
              "  'school,',\n",
              "  'bangkok,',\n",
              "  'thailand',\n",
              "  '',\n",
              "  'major:',\n",
              "  'mathematics-science',\n",
              "  'program',\n",
              "  '',\n",
              "  'work',\n",
              "  'experience',\n",
              "  'dec',\n",
              "  '2017',\n",
              "  '\\x96',\n",
              "  'apr',\n",
              "  '2021',\n",
              "  'teacher',\n",
              "  'assistant',\n",
              "  '(ta)',\n",
              "  'of',\n",
              "  'geography,',\n",
              "  'faculty',\n",
              "  'of',\n",
              "  'social',\n",
              "  'sciences,',\n",
              "  '',\n",
              "  'kasetsart',\n",
              "  'university,',\n",
              "  'thailand',\n",
              "  '\\x95',\n",
              "  'assist',\n",
              "  '',\n",
              "  'by',\n",
              "  '',\n",
              "  'preparing',\n",
              "  '',\n",
              "  'for',\n",
              "  '',\n",
              "  'teaching,',\n",
              "  '',\n",
              "  'check',\n",
              "  '',\n",
              "  'names,',\n",
              "  'and',\n",
              "  '',\n",
              "  'check',\n",
              "  'submissions.',\n",
              "  'of',\n",
              "  'students',\n",
              "  'each',\n",
              "  'week.',\n",
              "  '\\x95',\n",
              "  'assist',\n",
              "  'with',\n",
              "  'pre-test',\n",
              "  'and',\n",
              "  'post-test',\n",
              "  'checks',\n",
              "  'and',\n",
              "  'sum',\n",
              "  'the',\n",
              "  'scores',\n",
              "  'of',\n",
              "  'each',\n",
              "  'student',\n",
              "  'to',\n",
              "  'fill',\n",
              "  'in',\n",
              "  'excel.',\n",
              "  '\\x95',\n",
              "  'assist',\n",
              "  'by',\n",
              "  'preparing',\n",
              "  'the',\n",
              "  \"teacher's\",\n",
              "  'financial',\n",
              "  'documents',\n",
              "  'in',\n",
              "  'reimburse',\n",
              "  'tuition',\n",
              "  'fees',\n",
              "  'according',\n",
              "  'to',\n",
              "  'the',\n",
              "  'university',\n",
              "  'budget.',\n",
              "  'jul',\n",
              "  '2019',\n",
              "  'staff',\n",
              "  'at',\n",
              "  'aas-in-asia',\n",
              "  'conference,',\n",
              "  'royal',\n",
              "  'orchid',\n",
              "  'sheraton',\n",
              "  'hotel',\n",
              "  '&',\n",
              "  'towers,',\n",
              "  '',\n",
              "  'bangkok,',\n",
              "  'thailand',\n",
              "  '\\x95',\n",
              "  'register',\n",
              "  'for',\n",
              "  'the',\n",
              "  'event',\n",
              "  'and',\n",
              "  'provide',\n",
              "  'information',\n",
              "  'about',\n",
              "  'the',\n",
              "  'event',\n",
              "  'to',\n",
              "  'the',\n",
              "  'attendees.',\n",
              "  '\\x95',\n",
              "  'manage',\n",
              "  'meeting',\n",
              "  'schedules',\n",
              "  'for',\n",
              "  'each',\n",
              "  'session.',\n",
              "  'facilitate',\n",
              "  'the',\n",
              "  'speakers',\n",
              "  'and',\n",
              "  'coordinate',\n",
              "  'with',\n",
              "  'other',\n",
              "  'staff',\n",
              "  'to',\n",
              "  'solve',\n",
              "  'problems',\n",
              "  'such',\n",
              "  'as',\n",
              "  'technical',\n",
              "  'problems.',\n",
              "  '\\x95',\n",
              "  'ask',\n",
              "  'the',\n",
              "  'attendees',\n",
              "  'and',\n",
              "  'other',\n",
              "  'staff',\n",
              "  'about',\n",
              "  'the',\n",
              "  'feedback',\n",
              "  'of',\n",
              "  'the',\n",
              "  'event.',\n",
              "  'exchange',\n",
              "  'knowledge',\n",
              "  'and',\n",
              "  'ideas',\n",
              "  'about',\n",
              "  'each',\n",
              "  'session.',\n",
              "  '',\n",
              "  '----------------page',\n",
              "  '(0)',\n",
              "  'break----------------',\n",
              "  'other',\n",
              "  'experience',\n",
              "  'aug',\n",
              "  '2017',\n",
              "  '\\x96',\n",
              "  'apr',\n",
              "  '2021',\n",
              "  'member,',\n",
              "  'secretary',\n",
              "  'and',\n",
              "  'president',\n",
              "  'of',\n",
              "  'amateur',\n",
              "  'radio',\n",
              "  'club,',\n",
              "  'the',\n",
              "  '',\n",
              "  'club',\n",
              "  'for',\n",
              "  'students',\n",
              "  'who',\n",
              "  'interested',\n",
              "  'in',\n",
              "  'using',\n",
              "  'radio',\n",
              "  'communication,',\n",
              "  '',\n",
              "  'kasetsart',\n",
              "  'university,',\n",
              "  'thailand',\n",
              "  '\\x95',\n",
              "  'maintain',\n",
              "  'order',\n",
              "  'and',\n",
              "  'safety',\n",
              "  'in',\n",
              "  'various',\n",
              "  'activities',\n",
              "  'of',\n",
              "  'the',\n",
              "  'university',\n",
              "  'by',\n",
              "  'coordinating',\n",
              "  'with',\n",
              "  'the',\n",
              "  'security',\n",
              "  'officers',\n",
              "  'of',\n",
              "  'universities,',\n",
              "  'hospitals,',\n",
              "  'and',\n",
              "  'police',\n",
              "  'stations.',\n",
              "  '\\x95',\n",
              "  'prepare',\n",
              "  'various',\n",
              "  'government',\n",
              "  'documents,',\n",
              "  'take',\n",
              "  'notes',\n",
              "  'of',\n",
              "  'the',\n",
              "  'meeting,',\n",
              "  'prepare',\n",
              "  'and',\n",
              "  'follow-up',\n",
              "  'plans,',\n",
              "  'solve',\n",
              "  'specific',\n",
              "  'problems,',\n",
              "  'and',\n",
              "  'evaluate',\n",
              "  'the',\n",
              "  'performance',\n",
              "  'of',\n",
              "  'the',\n",
              "  'activities.',\n",
              "  '\\x95',\n",
              "  'coordinate',\n",
              "  'with',\n",
              "  'other',\n",
              "  'departments',\n",
              "  'of',\n",
              "  'the',\n",
              "  'university,',\n",
              "  'distribute',\n",
              "  'work',\n",
              "  'to',\n",
              "  'members,',\n",
              "  'change',\n",
              "  'the',\n",
              "  'activity',\n",
              "  'to',\n",
              "  'online,',\n",
              "  'and',\n",
              "  'solve',\n",
              "  'structural',\n",
              "  'problems',\n",
              "  'within',\n",
              "  'the',\n",
              "  'club.',\n",
              "  'aug',\n",
              "  '2018',\n",
              "  '\\x96',\n",
              "  'apr',\n",
              "  '2021',\n",
              "  'reception,',\n",
              "  'head',\n",
              "  'reception',\n",
              "  'and',\n",
              "  'head',\n",
              "  'security',\n",
              "  'of',\n",
              "  'the',\n",
              "  'social',\n",
              "  'sciences',\n",
              "  '',\n",
              "  'students',\n",
              "  'club,',\n",
              "  'faculty',\n",
              "  'of',\n",
              "  'social',\n",
              "  'sciences,',\n",
              "  'kasetsart',\n",
              "  'university,',\n",
              "  'thailand',\n",
              "  '\\x95',\n",
              "  'make',\n",
              "  'invitation',\n",
              "  'letter',\n",
              "  'and',\n",
              "  'speech',\n",
              "  'at',\n",
              "  'the',\n",
              "  'opening',\n",
              "  'and',\n",
              "  'closing',\n",
              "  'ceremony.',\n",
              "  '\\x95',\n",
              "  'make',\n",
              "  'a',\n",
              "  'plan',\n",
              "  'to',\n",
              "  'maintain',\n",
              "  'the',\n",
              "  'safety',\n",
              "  'and',\n",
              "  'orderliness',\n",
              "  'of',\n",
              "  'students',\n",
              "  'in',\n",
              "  'various',\n",
              "  'university',\n",
              "  'events.',\n",
              "  'and',\n",
              "  'ready',\n",
              "  'to',\n",
              "  'deal',\n",
              "  'with',\n",
              "  'unexpected',\n",
              "  'changes.',\n",
              "  'nov',\n",
              "  '2019',\n",
              "  '\\x96',\n",
              "  'jan',\n",
              "  '2020',\n",
              "  'staff',\n",
              "  'at',\n",
              "  'geography',\n",
              "  'relations',\n",
              "  'camp,',\n",
              "  'geography,',\n",
              "  'faculty',\n",
              "  'of',\n",
              "  'social',\n",
              "  'sciences,',\n",
              "  '',\n",
              "  'kasetsart',\n",
              "  'university,',\n",
              "  'thailand',\n",
              "  '\\x95',\n",
              "  'find',\n",
              "  'information,',\n",
              "  'plan',\n",
              "  'events,',\n",
              "  'manage',\n",
              "  'money,',\n",
              "  'coordinate',\n",
              "  'with',\n",
              "  'event',\n",
              "  'venues,',\n",
              "  'food,',\n",
              "  'and',\n",
              "  'accommodation.',\n",
              "  '\\x95',\n",
              "  'think',\n",
              "  'of',\n",
              "  'a',\n",
              "  'variety',\n",
              "  'of',\n",
              "  'activities',\n",
              "  'using',\n",
              "  'creativity',\n",
              "  'integration',\n",
              "  'with',\n",
              "  'geographic',\n",
              "  '',\n",
              "  'knowledge',\n",
              "  '',\n",
              "  'to',\n",
              "  '',\n",
              "  'be',\n",
              "  '',\n",
              "  'fun,',\n",
              "  '',\n",
              "  'creative,',\n",
              "  'engaging,',\n",
              "  'and',\n",
              "  'educational.',\n",
              "  '\\x95',\n",
              "  'adjust',\n",
              "  'activities',\n",
              "  'and',\n",
              "  'increase',\n",
              "  'flexibility',\n",
              "  'in',\n",
              "  'working',\n",
              "  'according',\n",
              "  'to',\n",
              "  'the',\n",
              "  'appropriate',\n",
              "  'situation.',\n",
              "  '',\n",
              "  'relevant',\n",
              "  'projects',\n",
              "  '\\x95',\n",
              "  'conducted',\n",
              "  'research',\n",
              "  'on',\n",
              "  'the',\n",
              "  'topic',\n",
              "  'of',\n",
              "  'prediction',\n",
              "  'of',\n",
              "  'the',\n",
              "  'residential',\n",
              "  'area',\n",
              "  'of',\n",
              "  'the',\n",
              "  'person',\n",
              "  'committing',\n",
              "  'a',\n",
              "  'burglary',\n",
              "  'offense',\n",
              "  'case',\n",
              "  'study:',\n",
              "  'area',\n",
              "  'of',\n",
              "  'responsibility',\n",
              "  'of',\n",
              "  'bang',\n",
              "  'khen',\n",
              "  'police',\n",
              "  'station,',\n",
              "  'it',\n",
              "  'is',\n",
              "  'research',\n",
              "  'conducted',\n",
              "  'to',\n",
              "  'predict',\n",
              "  'the',\n",
              "  'neighborhoods',\n",
              "  'of',\n",
              "  'people',\n",
              "  'who',\n",
              "  'commit',\n",
              "  'burglary',\n",
              "  'crimes',\n",
              "  'using',\n",
              "  'geographic',\n",
              "  'profiling',\n",
              "  'through',\n",
              "  'crimestat',\n",
              "  'iv.',\n",
              "  '\\x95',\n",
              "  'taught',\n",
              "  'juniors',\n",
              "  'in',\n",
              "  'the',\n",
              "  'soc',\n",
              "  'chorus',\n",
              "  'activity',\n",
              "  'group',\n",
              "  'to',\n",
              "  'kasetsart',\n",
              "  'university',\n",
              "  'participated',\n",
              "  'in',\n",
              "  'contest',\n",
              "  'which',\n",
              "  'we',\n",
              "  'won',\n",
              "  'the',\n",
              "  'third',\n",
              "  'prize',\n",
              "  'also',\n",
              "  'coordinate',\n",
              "  'with',\n",
              "  'government',\n",
              "  'agencies',\n",
              "  'in',\n",
              "  'the',\n",
              "  'university,',\n",
              "  'planning',\n",
              "  'a',\n",
              "  'training,',\n",
              "  'strategy',\n",
              "  'originality',\n",
              "  'in',\n",
              "  'choreography',\n",
              "  'made',\n",
              "  'sure',\n",
              "  'walking',\n",
              "  'styles',\n",
              "  'would',\n",
              "  'be',\n",
              "  'consistent',\n",
              "  'with',\n",
              "  'the',\n",
              "  'lyrics',\n",
              "  'and',\n",
              "  'gestures',\n",
              "  'of',\n",
              "  'the',\n",
              "  'cheerleaders,',\n",
              "  'including',\n",
              "  'dealing',\n",
              "  'with',\n",
              "  'sudden',\n",
              "  'changes',\n",
              "  'in',\n",
              "  'the',\n",
              "  'work',\n",
              "  'and',\n",
              "  'managing',\n",
              "  'time',\n",
              "  'under',\n",
              "  'the',\n",
              "  'pressure',\n",
              "  'of',\n",
              "  'limited',\n",
              "  'time.',\n",
              "  '',\n",
              "  'additional',\n",
              "  'information',\n",
              "  'microsoft',\n",
              "  'office',\n",
              "  '(intermediate)',\n",
              "  'arcgis,',\n",
              "  'qgis,',\n",
              "  'and',\n",
              "  'envi',\n",
              "  '(intermediate)',\n",
              "  'english',\n",
              "  '(intermediate)',\n",
              "  'spss,',\n",
              "  'snap,',\n",
              "  'and',\n",
              "  'ecognition',\n",
              "  '(basic)',\n",
              "  'creative',\n",
              "  'content',\n",
              "  'and',\n",
              "  'storytelling',\n",
              "  '(basic)',\n",
              "  'steam4innovator',\n",
              "  '(basic)',\n",
              "  '----------------page',\n",
              "  '(1)',\n",
              "  'break----------------',\n",
              "  '']}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGNfe0uJzQ0U",
        "outputId": "fd89baff-e532-46fa-c3af-aae295e55ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertForTokenClassification: ['pre_classifier.weight', 'pre_classifier.bias']\n",
            "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "##first\n",
        "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", num_labels=len(dm.unique_entities), id2label=dm.id2label, label2id=dm.label2id,ignore_mismatched_sizes=True)\n",
        "#model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(dm.unique_entities), id2label=dm.id2label, label2id=dm.label2id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"drive/MyDrive/PersonEntModelTest2\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"drive/MyDrive/PersonEntModelTest2\")\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "aiYFtJZaT-Hy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0Yq40zhosUNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVmVPr_hndUi",
        "outputId": "e9510821-215a-4072-a7d2-06b439a57a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in drive/MyDrive/PersonEntModel3/config.json\n",
            "Model weights saved in drive/MyDrive/PersonEntModel3/pytorch_model.bin\n",
            "tokenizer config file saved in drive/MyDrive/PersonEntModel3/tokenizer_config.json\n",
            "Special tokens file saved in drive/MyDrive/PersonEntModel3/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('drive/MyDrive/PersonEntModel3/tokenizer_config.json',\n",
              " 'drive/MyDrive/PersonEntModel3/special_tokens_map.json',\n",
              " 'drive/MyDrive/PersonEntModel3/vocab.txt',\n",
              " 'drive/MyDrive/PersonEntModel3/added_tokens.json',\n",
              " 'drive/MyDrive/PersonEntModel3/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e4fa61e098e14cf8a591d9c99c0cc9bf",
            "ed787a9800fa44a8a8c9e7a70c84c150",
            "3de632eb7f874506a119a4eca07beeab",
            "ccc838124fe64373879d59498d79e551",
            "40df6057e3a749c5b8e9fe0a084c098c",
            "830b5c8168dc40e7887eae020701413b",
            "25bed9456b10491592c4e491fd47fee3",
            "862077e0b81e42c9bb047670e9a32f2e",
            "26e52ce991e64b3da949ce7583150997",
            "daa507a5094d4e45b79bd351d5cf63f6",
            "04bcbc2c6f744a049b80e34d8a0d7d52"
          ]
        },
        "id": "KXN0LU78z3BS",
        "outputId": "15fc8849-9479-4e8d-e412-17d07fdd6009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4fa61e098e14cf8a591d9c99c0cc9bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 95\n",
            "  Num Epochs = 40\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 240\n",
            "  Number of trainable parameters = 66365187\n",
            "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [240/240 3:47:23, Epoch 40/40]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.001927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000028</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 95\n",
            "  Batch size = 16\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Configuration saved in drive/MyDrive/PersonEntModelTest3/config.json\n",
            "Model weights saved in drive/MyDrive/PersonEntModelTest3/pytorch_model.bin\n",
            "tokenizer config file saved in drive/MyDrive/PersonEntModelTest3/tokenizer_config.json\n",
            "Special tokens file saved in drive/MyDrive/PersonEntModelTest3/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('drive/MyDrive/PersonEntModelTest3/tokenizer_config.json',\n",
              " 'drive/MyDrive/PersonEntModelTest3/special_tokens_map.json',\n",
              " 'drive/MyDrive/PersonEntModelTest3/vocab.txt',\n",
              " 'drive/MyDrive/PersonEntModelTest3/added_tokens.json',\n",
              " 'drive/MyDrive/PersonEntModelTest3/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=40,\n",
        "    weight_decay=0.02,\n",
        ")\n",
        "\n",
        "train_ds = dm.as_hf_dataset(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=train_ds, # eval on training set! ONLY for DEMO!!\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained(\"drive/MyDrive/PersonEntModelTest3\")\n",
        "tokenizer.save_pretrained(\"drive/MyDrive/PersonEntModelTest3\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZHPY-o05La8R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dftest = pd.read_csv('textInResume.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dfName = pd.read_csv('nameinresume.csv')"
      ],
      "metadata": {
        "id": "nZBYpsk4DbiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstName = (dfName['firstname'].values+' '+dfName['lastname'].values).tolist()\n",
        "lstName"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "B1GjjPVODhkE",
        "outputId": "a8356247-be0d-4ca0-c94e-39751ec545b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5fe4ac6adbbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdfName\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'firstname'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdfName\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lastname'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlstName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dfName' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Xu0-B0aiMKmE"
      },
      "outputs": [],
      "source": [
        "lsttest = dftest['text'].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "odAHCbMzMP-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada884c2-5071-4ad1-941a-3d4932b4c2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            " ()\n",
            "[{'entity_group': 'PERSON', 'score': 0.95196307, 'word': 'nisaratwisesbantao', 'start': 0, 'end': 18}]\n",
            " | nisaratwisesbantao (nisaratwisesbantao)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99890757, 'word': 'th', 'start': 178, 'end': 180}, {'entity_group': 'PERSON', 'score': 0.999071, 'word': '##ira', 'start': 180, 'end': 183}, {'entity_group': 'PERSON', 'score': 0.9884774, 'word': '##wit jirarungro', 'start': 183, 'end': 197}]\n",
            " | th | ira | wit jirarungro (thirawit jirarungro)\n",
            "[]\n",
            " ()\n",
            "[{'entity_group': 'PERSON', 'score': 0.99921846, 'word': 'than', 'start': 0, 'end': 4}, {'entity_group': 'PERSON', 'score': 0.9822139, 'word': '##chanok watcharakitphokin', 'start': 4, 'end': 28}]\n",
            " | than | chanok watcharakitphokin (thanchanok watcharakitphokin)\n",
            "[{'entity_group': 'PERSON', 'score': 0.89714295, 'word': 'nat', 'start': 612, 'end': 615}, {'entity_group': 'PERSON', 'score': 0.7495198, 'word': '##anop pimonsathi', 'start': 615, 'end': 630}, {'entity_group': 'PERSON', 'score': 0.5261133, 'word': '##monsathi', 'start': 1405, 'end': 1413}]\n",
            " | nat | anop pimonsathi | monsathi (natanop pimonsathimonsathi)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9989587, 'word': 'patch', 'start': 0, 'end': 5}, {'entity_group': 'PERSON', 'score': 0.99277496, 'word': '##araya anuntasink', 'start': 5, 'end': 21}]\n",
            " | patch | araya anuntasink (patcharaya anuntasink)\n",
            "[{'entity_group': 'PERSON', 'score': 0.96479666, 'word': 'jarrukorn pens', 'start': 4, 'end': 18}]\n",
            " | jarrukorn pens (jarrukorn pens)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9979773, 'word': 'pr', 'start': 0, 'end': 2}, {'entity_group': 'PERSON', 'score': 0.9815075, 'word': '##avittra vimonworachort', 'start': 2, 'end': 24}]\n",
            " | pr | avittra vimonworachort (pravittra vimonworachort)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9278046, 'word': '##ad', 'start': 178, 'end': 180}, {'entity_group': 'PERSON', 'score': 0.5820685, 'word': 'nat', 'start': 323, 'end': 326}]\n",
            " | ad | nat (adnat)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99841976, 'word': 'pu', 'start': 0, 'end': 2}, {'entity_group': 'PERSON', 'score': 0.99335384, 'word': '##rawat ruangsri', 'start': 2, 'end': 16}]\n",
            " | pu | rawat ruangsri (purawat ruangsri)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9974191, 'word': 'nut', 'start': 2, 'end': 5}, {'entity_group': 'PERSON', 'score': 0.9352042, 'word': '##ratanon maha', 'start': 5, 'end': 17}]\n",
            " | nut | ratanon maha (nutratanon maha)\n",
            "[{'entity_group': 'PERSON', 'score': 0.999226, 'word': 'kit', 'start': 0, 'end': 3}, {'entity_group': 'PERSON', 'score': 0.989301, 'word': '##isak thossaensin', 'start': 3, 'end': 19}]\n",
            " | kit | isak thossaensin (kitisak thossaensin)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9988808, 'word': 'sue', 'start': 15, 'end': 18}, {'entity_group': 'PERSON', 'score': 0.99469614, 'word': '##btas limsaihua', 'start': 18, 'end': 32}]\n",
            " | sue | btas limsaihua (suebtas limsaihua)\n",
            "[{'entity_group': 'PERSON', 'score': 0.95091426, 'word': 'navara sirijarus', 'start': 1888, 'end': 1904}, {'entity_group': 'PERSON', 'score': 0.908993, 'word': 'navara sirijarusvon', 'start': 2004, 'end': 2023}]\n",
            " | navara sirijarus | navara sirijarusvon (navara sirijarusnavara sirijarusvon)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9983693, 'word': 'cha', 'start': 166, 'end': 169}, {'entity_group': 'PERSON', 'score': 0.9893121, 'word': '##yanit sripradit', 'start': 169, 'end': 184}]\n",
            " | cha | yanit sripradit (chayanit sripradit)\n",
            "[{'entity_group': 'PERSON', 'score': 0.997538, 'word': 'pi', 'start': 0, 'end': 2}, {'entity_group': 'PERSON', 'score': 0.94968987, 'word': '##mnares put', 'start': 2, 'end': 12}]\n",
            " | pi | mnares put (pimnares put)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9872923, 'word': 'saran thitawiriyayo', 'start': 0, 'end': 19}, {'entity_group': 'PERSON', 'score': 0.98640895, 'word': '##wiriya', 'start': 135, 'end': 141}, {'entity_group': 'PERSON', 'score': 0.8866375, 'word': 'th', 'start': 182, 'end': 184}, {'entity_group': 'PERSON', 'score': 0.9889055, 'word': '##wiriya', 'start': 187, 'end': 193}]\n",
            " | saran thitawiriyayo | wiriya | th | wiriya (saran thitawiriyayowiriyathwiriya)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99841535, 'word': 'pak', 'start': 1, 'end': 4}, {'entity_group': 'PERSON', 'score': 0.97825146, 'word': '##poom po', 'start': 4, 'end': 11}, {'entity_group': 'PERSON', 'score': 0.57795113, 'word': '##m', 'start': 77, 'end': 78}]\n",
            " | pak | poom po | m (pakpoom pom)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99909985, 'word': 'kan', 'start': 3, 'end': 6}, {'entity_group': 'PERSON', 'score': 0.99831915, 'word': '##ittha setthapitayakul', 'start': 6, 'end': 27}]\n",
            " | kan | ittha setthapitayakul (kanittha setthapitayakul)\n",
            "[{'entity_group': 'PERSON', 'score': 0.78337425, 'word': '##nu', 'start': 142, 'end': 144}, {'entity_group': 'PERSON', 'score': 0.9669194, 'word': 'duangdenphatsawr', 'start': 147, 'end': 163}]\n",
            " | nu | duangdenphatsawr (nuduangdenphatsawr)\n",
            "[{'entity_group': 'PERSON', 'score': 0.95738655, 'word': 'pawith panyasirikul', 'start': 0, 'end': 19}]\n",
            " | pawith panyasirikul (pawith panyasirikul)\n",
            "[{'entity_group': 'PERSON', 'score': 0.78144526, 'word': '##tou', 'start': 5, 'end': 8}, {'entity_group': 'PERSON', 'score': 0.9704342, 'word': 'supalanunt', 'start': 11, 'end': 21}]\n",
            " | tou | supalanunt (tousupalanunt)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9794478, 'word': 'aus', 'start': 0, 'end': 3}, {'entity_group': 'PERSON', 'score': 0.6955068, 'word': '##ch mas', 'start': 7, 'end': 13}]\n",
            " | aus | ch mas (ausch mas)\n",
            "[{'entity_group': 'PERSON', 'score': 0.93094194, 'word': 'tee', 'start': 1438, 'end': 1441}, {'entity_group': 'PERSON', 'score': 0.99666, 'word': '##rata', 'start': 1441, 'end': 1445}, {'entity_group': 'PERSON', 'score': 0.9923582, 'word': 'jearapagano', 'start': 1448, 'end': 1459}]\n",
            " | tee | rata | jearapagano (teeratajearapagano)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99897695, 'word': 'pr', 'start': 38, 'end': 40}, {'entity_group': 'PERSON', 'score': 0.9822852, 'word': '##atan srikamonpattana', 'start': 40, 'end': 60}]\n",
            " | pr | atan srikamonpattana (pratan srikamonpattana)\n",
            "[]\n",
            " ()\n",
            "[{'entity_group': 'PERSON', 'score': 0.9991743, 'word': 'cha', 'start': 0, 'end': 3}, {'entity_group': 'PERSON', 'score': 0.9665534, 'word': '##yanon juntarapartsavorn', 'start': 3, 'end': 26}]\n",
            " | cha | yanon juntarapartsavorn (chayanon juntarapartsavorn)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99865794, 'word': 'na', 'start': 0, 'end': 2}, {'entity_group': 'PERSON', 'score': 0.9772068, 'word': '##rurong saeheng', 'start': 2, 'end': 16}]\n",
            " | na | rurong saeheng (narurong saeheng)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9529427, 'word': 'su', 'start': 22, 'end': 24}, {'entity_group': 'PERSON', 'score': 0.6928718, 'word': '##pachai sum', 'start': 24, 'end': 34}, {'entity_group': 'PERSON', 'score': 0.8496196, 'word': 'su', 'start': 530, 'end': 532}, {'entity_group': 'PERSON', 'score': 0.87289065, 'word': '##pachai sumeteena', 'start': 532, 'end': 548}]\n",
            " | su | pachai sum | su | pachai sumeteena (supachai sumsupachai sumeteena)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99905187, 'word': 'it', 'start': 0, 'end': 2}, {'entity_group': 'PERSON', 'score': 0.9977604, 'word': '##tic', 'start': 2, 'end': 5}, {'entity_group': 'PERSON', 'score': 0.99369437, 'word': '##hote sornmeeth', 'start': 5, 'end': 19}]\n",
            " | it | tic | hote sornmeeth (ittichote sornmeeth)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99925166, 'word': 'kam', 'start': 0, 'end': 3}, {'entity_group': 'PERSON', 'score': 0.9985009, 'word': '##olchai suebnipon', 'start': 3, 'end': 19}]\n",
            " | kam | olchai suebnipon (kamolchai suebnipon)\n",
            "[{'entity_group': 'PERSON', 'score': 0.91587555, 'word': 'surasak intas', 'start': 6, 'end': 19}]\n",
            " | surasak intas (surasak intas)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99921346, 'word': 'nat', 'start': 0, 'end': 3}, {'entity_group': 'PERSON', 'score': 0.98840505, 'word': '##tachai noogure', 'start': 3, 'end': 17}]\n",
            " | nat | tachai noogure (nattachai noogure)\n",
            "[{'entity_group': 'PERSON', 'score': 0.82486653, 'word': 'pawasakari', 'start': 1869, 'end': 1879}, {'entity_group': 'PERSON', 'score': 0.99498004, 'word': 'sa', 'start': 1909, 'end': 1911}, {'entity_group': 'PERSON', 'score': 0.99092937, 'word': '##ksorn pawasakarin', 'start': 1911, 'end': 1928}]\n",
            " | pawasakari | sa | ksorn pawasakarin (pawasakarisaksorn pawasakarin)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99839205, 'word': 'na', 'start': 0, 'end': 2}, {'entity_group': 'PERSON', 'score': 0.9460466, 'word': '##vapuvadol uraikul', 'start': 2, 'end': 19}]\n",
            " | na | vapuvadol uraikul (navapuvadol uraikul)\n",
            "[]\n",
            " ()\n",
            "[{'entity_group': 'PERSON', 'score': 0.9835242, 'word': 'set', 'start': 27, 'end': 30}, {'entity_group': 'PERSON', 'score': 0.829153, 'word': '##wuth kangwansakol', 'start': 33, 'end': 50}]\n",
            " | set | wuth kangwansakol (setwuth kangwansakol)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99899215, 'word': 'rat', 'start': 5, 'end': 8}, {'entity_group': 'PERSON', 'score': 0.9944767, 'word': '##chanok angkhahad', 'start': 8, 'end': 24}]\n",
            " | rat | chanok angkhahad (ratchanok angkhahad)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99810374, 'word': 've', 'start': 0, 'end': 2}, {'entity_group': 'PERSON', 'score': 0.979174, 'word': '##erachaimitmorn', 'start': 2, 'end': 16}]\n",
            " | ve | erachaimitmorn (veerachaimitmorn)\n",
            "[{'entity_group': 'PERSON', 'score': 0.96366704, 'word': '##chi', 'start': 538, 'end': 541}, {'entity_group': 'PERSON', 'score': 0.9636, 'word': 'korsanankittip', 'start': 544, 'end': 558}]\n",
            " | chi | korsanankittip (chikorsanankittip)\n",
            "[{'entity_group': 'PERSON', 'score': 0.93130857, 'word': 'suphakit sangthong', 'start': 0, 'end': 18}, {'entity_group': 'PERSON', 'score': 0.98708963, 'word': 'suphakit', 'start': 830, 'end': 838}]\n",
            " | suphakit sangthong | suphakit (suphakit sangthongsuphakit)\n",
            "[{'entity_group': 'PERSON', 'score': 0.89227253, 'word': 'porameht khumsom', 'start': 21, 'end': 38}]\n",
            " | porameht khumsom (porameht khumsom)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9991049, 'word': 'ji', 'start': 0, 'end': 2}, {'entity_group': 'PERSON', 'score': 0.990087, 'word': '##tawat chanpraneet', 'start': 2, 'end': 19}]\n",
            " | ji | tawat chanpraneet (jitawat chanpraneet)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9798007, 'word': 'rajawana', 'start': 1063, 'end': 1071}]\n",
            " | rajawana (rajawana)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9911462, 'word': 'suwan panjanapongchai', 'start': 1, 'end': 22}]\n",
            " | suwan panjanapongchai (suwan panjanapongchai)\n",
            "[{'entity_group': 'PERSON', 'score': 0.82486653, 'word': 'pawasakari', 'start': 1869, 'end': 1879}, {'entity_group': 'PERSON', 'score': 0.99498004, 'word': 'sa', 'start': 1909, 'end': 1911}, {'entity_group': 'PERSON', 'score': 0.99092937, 'word': '##ksorn pawasakarin', 'start': 1911, 'end': 1928}]\n",
            " | pawasakari | sa | ksorn pawasakarin (pawasakarisaksorn pawasakarin)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99836236, 'word': 'kam', 'start': 0, 'end': 3}, {'entity_group': 'PERSON', 'score': 0.9329588, 'word': '##olwan penpetch', 'start': 3, 'end': 17}]\n",
            " | kam | olwan penpetch (kamolwan penpetch)\n",
            "[]\n",
            " ()\n",
            "[{'entity_group': 'PERSON', 'score': 0.98369205, 'word': 'th', 'start': 1016, 'end': 1018}, {'entity_group': 'PERSON', 'score': 0.9936492, 'word': 'chanon sattrupinat', 'start': 1019, 'end': 1037}]\n",
            " | th | chanon sattrupinat (thchanon sattrupinat)\n",
            "[{'entity_group': 'PERSON', 'score': 0.83146995, 'word': 'th', 'start': 1560, 'end': 1562}, {'entity_group': 'PERSON', 'score': 0.736395, 'word': '##ien', 'start': 1562, 'end': 1565}]\n",
            " | th | ien (thien)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9743955, 'word': 'k', 'start': 952, 'end': 953}, {'entity_group': 'PERSON', 'score': 0.85298836, 'word': '##rittapad harn', 'start': 953, 'end': 966}]\n",
            " | k | rittapad harn (krittapad harn)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9457858, 'word': 'put phapon', 'start': 4, 'end': 14}]\n",
            " | put phapon (put phapon)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9992311, 'word': 'such', 'start': 4, 'end': 8}, {'entity_group': 'PERSON', 'score': 0.98849565, 'word': '##it rojanapatanasom', 'start': 8, 'end': 26}]\n",
            " | such | it rojanapatanasom (suchit rojanapatanasom)\n",
            "[{'entity_group': 'PERSON', 'score': 0.7314812, 'word': '##krabang', 'start': 789, 'end': 796}, {'entity_group': 'PERSON', 'score': 0.96809614, 'word': 'nat', 'start': 797, 'end': 800}, {'entity_group': 'PERSON', 'score': 0.87439364, 'word': '##ta', 'start': 800, 'end': 802}, {'entity_group': 'PERSON', 'score': 0.8362452, 'word': '##woot dendu', 'start': 802, 'end': 812}]\n",
            " | krabang | nat | ta | woot dendu (krabangnattawoot dendu)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99928856, 'word': 'tha', 'start': 0, 'end': 3}, {'entity_group': 'PERSON', 'score': 0.9955819, 'word': '##rathep chuayrod', 'start': 3, 'end': 18}]\n",
            " | tha | rathep chuayrod (tharathep chuayrod)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9991092, 'word': 'non', 'start': 23, 'end': 26}, {'entity_group': 'PERSON', 'score': 0.94905305, 'word': '##thawat aphiwong', 'start': 26, 'end': 41}]\n",
            " | non | thawat aphiwong (nonthawat aphiwong)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99909127, 'word': 'ti', 'start': 6, 'end': 8}, {'entity_group': 'PERSON', 'score': 0.9284689, 'word': '##titab srisookco', 'start': 8, 'end': 23}]\n",
            " | ti | titab srisookco (tititab srisookco)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9989994, 'word': 'nut', 'start': 5, 'end': 8}, {'entity_group': 'PERSON', 'score': 0.9981103, 'word': '##tamol janmanee', 'start': 8, 'end': 22}]\n",
            " | nut | tamol janmanee (nuttamol janmanee)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9977075, 'word': 'api', 'start': 841, 'end': 844}, {'entity_group': 'PERSON', 'score': 0.98738813, 'word': '##nop soisuwan', 'start': 844, 'end': 856}]\n",
            " | api | nop soisuwan (apinop soisuwan)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9990376, 'word': 'k', 'start': 0, 'end': 1}, {'entity_group': 'PERSON', 'score': 0.9989931, 'word': '##rit', 'start': 1, 'end': 4}, {'entity_group': 'PERSON', 'score': 0.9961877, 'word': '##danai peerapolchaikul', 'start': 4, 'end': 25}]\n",
            " | k | rit | danai peerapolchaikul (kritdanai peerapolchaikul)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9992544, 'word': 'kit', 'start': 0, 'end': 3}, {'entity_group': 'PERSON', 'score': 0.99341786, 'word': '##tiphong chankong', 'start': 3, 'end': 19}]\n",
            " | kit | tiphong chankong (kittiphong chankong)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99066633, 'word': 'pak', 'start': 4, 'end': 7}, {'entity_group': 'PERSON', 'score': 0.90555537, 'word': '##avich veeranarapanich', 'start': 7, 'end': 28}]\n",
            " | pak | avich veeranarapanich (pakavich veeranarapanich)\n",
            "[{'entity_group': 'PERSON', 'score': 0.92394316, 'word': 'pawana', 'start': 1166, 'end': 1172}]\n",
            " | pawana (pawana)\n",
            "[{'entity_group': 'PERSON', 'score': 0.87075406, 'word': '.', 'start': 2, 'end': 3}, {'entity_group': 'PERSON', 'score': 0.9891664, 'word': 'pisarnwate jitvimol', 'start': 4, 'end': 23}]\n",
            " | . | pisarnwate jitvimol (.pisarnwate jitvimol)\n",
            "[{'entity_group': 'PERSON', 'score': 0.88264084, 'word': '##hat', 'start': 76, 'end': 79}, {'entity_group': 'PERSON', 'score': 0.9412353, 'word': '##tharachon pilikamin', 'start': 79, 'end': 99}]\n",
            " | hat | tharachon pilikamin (hattharachon pilikamin)\n",
            "[{'entity_group': 'PERSON', 'score': 0.99934465, 'word': 'peer', 'start': 1, 'end': 5}, {'entity_group': 'PERSON', 'score': 0.99868256, 'word': '##awish tawantarong', 'start': 5, 'end': 22}]\n",
            " | peer | awish tawantarong (peerawish tawantarong)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9981717, 'word': 'than', 'start': 215, 'end': 219}, {'entity_group': 'PERSON', 'score': 0.9814501, 'word': '##aphon wichuladawut', 'start': 219, 'end': 237}]\n",
            " | than | aphon wichuladawut (thanaphon wichuladawut)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9942894, 'word': '.', 'start': 3, 'end': 4}, {'entity_group': 'PERSON', 'score': 0.99870706, 'word': 'ko', 'start': 5, 'end': 7}, {'entity_group': 'PERSON', 'score': 0.9976367, 'word': '##mson packdeearporn', 'start': 7, 'end': 25}]\n",
            " | . | ko | mson packdeearporn (.komson packdeearporn)\n",
            "[{'entity_group': 'PERSON', 'score': 0.85020304, 'word': 'ektanat pupat', 'start': 0, 'end': 13}]\n",
            " | ektanat pupat (ektanat pupat)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9990971, 'word': 'tha', 'start': 0, 'end': 3}, {'entity_group': 'PERSON', 'score': 0.91710395, 'word': '##paneesuklapkit', 'start': 3, 'end': 17}, {'entity_group': 'PERSON', 'score': 0.9970571, 'word': 'tha', 'start': 275, 'end': 278}, {'entity_group': 'PERSON', 'score': 0.95859224, 'word': '##panee su', 'start': 278, 'end': 286}, {'entity_group': 'PERSON', 'score': 0.5316023, 'word': '##pan', 'start': 758, 'end': 761}]\n",
            " | tha | paneesuklapkit | tha | panee su | pan (thapaneesuklapkitthapanee supan)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9473892, 'word': 'amornmaneekul', 'start': 158, 'end': 171}, {'entity_group': 'PERSON', 'score': 0.9959072, 'word': 'pi', 'start': 927, 'end': 929}, {'entity_group': 'PERSON', 'score': 0.9978854, 'word': '##yat', 'start': 929, 'end': 932}, {'entity_group': 'PERSON', 'score': 0.51007485, 'word': 'pia', 'start': 936, 'end': 939}]\n",
            " | amornmaneekul | pi | yat | pia (amornmaneekulpiyatpia)\n",
            "[{'entity_group': 'PERSON', 'score': 0.998566, 'word': 'ha', 'start': 49, 'end': 51}, {'entity_group': 'PERSON', 'score': 0.91301215, 'word': '##ruthai jankrajun', 'start': 51, 'end': 67}]\n",
            " | ha | ruthai jankrajun (haruthai jankrajun)\n",
            "[{'entity_group': 'PERSON', 'score': 0.5552786, 'word': '##ku', 'start': 172, 'end': 174}]\n",
            " | ku (ku)\n",
            "[]\n",
            " ()\n",
            "[{'entity_group': 'PERSON', 'score': 0.9135668, 'word': 'arthit thetkham', 'start': 0, 'end': 15}]\n",
            " | arthit thetkham (arthit thetkham)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9946748, 'word': 'yo', 'start': 268, 'end': 270}, {'entity_group': 'PERSON', 'score': 0.99477106, 'word': '##nlada nedluecha', 'start': 270, 'end': 285}, {'entity_group': 'PERSON', 'score': 0.79335266, 'word': '##harnjamsaiwitta', 'start': 1749, 'end': 1764}]\n",
            " | yo | nlada nedluecha | harnjamsaiwitta (yonlada nedluechaharnjamsaiwitta)\n",
            "[{'entity_group': 'PERSON', 'score': 0.544389, 'word': '##bunr', 'start': 257, 'end': 261}]\n",
            " | bunr (bunr)\n",
            "[{'entity_group': 'PERSON', 'score': 0.78781813, 'word': '##nakorn rattanabu', 'start': 351, 'end': 367}]\n",
            " | nakorn rattanabu (nakorn rattanabu)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9990289, 'word': 'nat', 'start': 0, 'end': 3}, {'entity_group': 'PERSON', 'score': 0.9935541, 'word': '##akit lalitputticho', 'start': 3, 'end': 21}]\n",
            " | nat | akit lalitputticho (natakit lalitputticho)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9303617, 'word': 'th', 'start': 30, 'end': 32}, {'entity_group': 'PERSON', 'score': 0.574832, 'word': '##um', 'start': 32, 'end': 34}, {'entity_group': 'PERSON', 'score': 0.8058254, 'word': '##marat pakla', 'start': 34, 'end': 45}, {'entity_group': 'PERSON', 'score': 0.6023137, 'word': '##t', 'start': 272, 'end': 273}, {'entity_group': 'PERSON', 'score': 0.5670255, 'word': '##la', 'start': 277, 'end': 279}]\n",
            " | th | um | marat pakla | t | la (thummarat paklatla)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9992169, 'word': 'pat', 'start': 1, 'end': 4}, {'entity_group': 'PERSON', 'score': 0.97716624, 'word': '##ipan mata resume', 'start': 4, 'end': 20}]\n",
            " | pat | ipan mata resume (patipan mata resume)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9989642, 'word': 'tan', 'start': 0, 'end': 3}, {'entity_group': 'PERSON', 'score': 0.9220664, 'word': '##akrid chanburi', 'start': 3, 'end': 17}]\n",
            " | tan | akrid chanburi (tanakrid chanburi)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9908965, 'word': 'sara', 'start': 919, 'end': 923}, {'entity_group': 'PERSON', 'score': 0.9849372, 'word': '##krit thahanthai', 'start': 923, 'end': 938}, {'entity_group': 'PERSON', 'score': 0.9982589, 'word': 'sara', 'start': 1427, 'end': 1431}, {'entity_group': 'PERSON', 'score': 0.9940756, 'word': '##krit thahanthai', 'start': 1431, 'end': 1446}]\n",
            " | sara | krit thahanthai | sara | krit thahanthai (sarakrit thahanthaisarakrit thahanthai)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9992422, 'word': 'na', 'start': 0, 'end': 2}, {'entity_group': 'PERSON', 'score': 0.9971102, 'word': '##ruedom kiatikoon', 'start': 2, 'end': 18}]\n",
            " | na | ruedom kiatikoon (naruedom kiatikoon)\n",
            "[{'entity_group': 'PERSON', 'score': 0.89487576, 'word': 'pattarapon buathong', 'start': 0, 'end': 19}]\n",
            " | pattarapon buathong (pattarapon buathong)\n",
            "[{'entity_group': 'PERSON', 'score': 0.9985917, 'word': 'wit', 'start': 0, 'end': 3}, {'entity_group': 'PERSON', 'score': 0.82916117, 'word': '##tawat', 'start': 3, 'end': 8}]\n",
            " | wit | tawat (wittawat)\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\") # pass device=0 if using gpu\n",
        "i=0\n",
        "num=0\n",
        "for test in lsttest:\n",
        "  ent = pipe(test)\n",
        "  txtName = ''\n",
        "  txt =''\n",
        "  for val in ent:\n",
        "    txtName = txtName+\" | \" + val['word'].replace('##','')\n",
        "    txt = txt+val['word'].replace('##','')\n",
        "  print( ent)\n",
        "  print(txtName,'('+txt+')')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  for name in lstName:\n",
        "    if hex(id(name))==hex(id(txt)):\n",
        "       num=num+1 \n",
        "       print(num)\n",
        "    break\n",
        "  i= i+1"
      ],
      "metadata": {
        "id": "B85FB5kzm6QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = (num/87)*100\n",
        "print(num,acc)\n",
        "num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTDOxqAJFj4k",
        "outputId": "1d66f5b2-2987-4575-dc0a-d1621758deeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"\"\"Busaba Supasawat        profile a self-starter and quick learner with two-year experience   in financial industry. versatile  skill  sets including problem solving,  quantitative and analytical skills. seeking for career in financial industry where i can utilize and further  develop my  skills  and knowledge to   bring innovative solutions helping company to achieve the company goal.   contact phone: 0838523456  email: kanittha.se@hotmail.com  address: 118/208 condo vtara 36,  soi saen-sa-buy, rama iv rd.,  phra-khanong, klong-toei, bangkok  10110  work experience structured products development - bualuang securities may 2020  present ? develop and conduct structured product pricing and other relevant areas ? analyze data to efficiently expand client-based market ? provide market outlook and trading strategies of structured note to the sales team ? create new tools used to assist the sales team  ? handle project for improving current working flow  derivatives analyst - citibank n.a. april 2019  april 2020 ? review and improve the existing process to increase efficiency by applying automation process ? process settlement and confirmations via verbal and electronics means for derivative and structures product ? handle exceptions and providing constructive solutions to resolve issues  onsite analyst - central group online september 2018  march 2019 ? develop excel spreadsheet to improve day-to-day job ? perform ad hoc analysis to identify business issues and draw recommendation in marketing performance management  ? develop performance tracking on google tag manager on website education tsinghua university, beijing, china [m.sc.] aug 2016  july 2018  ? management science and engineering (mse) program ? department of industrial engineering chulalongkorn university [b. eng.] may 2012  may 2016 ? department of chemical engineering  ? awarded second-class honors computer skills ? vba in excel (advance), python (intermediate), sql (basic) ? excellent working knowledge for tableau, power bi certificate  ic complex 1 language skills  chinese (hsk5)  english (intermediate)   ----------------page (0) break---------------- \"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyZmBJOz94_H",
        "outputId": "225b4737-dd56-4431-c07a-cd2eea688d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PERSON',\n",
              "  'score': 0.9372089,\n",
              "  'word': 'bus',\n",
              "  'start': 0,\n",
              "  'end': 3},\n",
              " {'entity_group': 'PERSON',\n",
              "  'score': 0.64176697,\n",
              "  'word': '##aba',\n",
              "  'start': 3,\n",
              "  'end': 6},\n",
              " {'entity_group': 'PERSON',\n",
              "  'score': 0.8098486,\n",
              "  'word': 'su',\n",
              "  'start': 7,\n",
              "  'end': 9},\n",
              " {'entity_group': 'PERSON',\n",
              "  'score': 0.8008657,\n",
              "  'word': '##pas',\n",
              "  'start': 9,\n",
              "  'end': 12},\n",
              " {'entity_group': 'PERSON',\n",
              "  'score': 0.7050145,\n",
              "  'word': '##awa',\n",
              "  'start': 12,\n",
              "  'end': 15}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJmVkxn1z-5K",
        "outputId": "e47388a2-157d-45f0-ab9a-90beca06059a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PERSON',\n",
              "  'score': 0.9372089,\n",
              "  'word': 'bus',\n",
              "  'start': 0,\n",
              "  'end': 3},\n",
              " {'entity_group': 'PERSON',\n",
              "  'score': 0.64176697,\n",
              "  'word': '##aba',\n",
              "  'start': 3,\n",
              "  'end': 6},\n",
              " {'entity_group': 'PERSON',\n",
              "  'score': 0.8098486,\n",
              "  'word': 'su',\n",
              "  'start': 7,\n",
              "  'end': 9},\n",
              " {'entity_group': 'PERSON',\n",
              "  'score': 0.8008657,\n",
              "  'word': '##pas',\n",
              "  'start': 9,\n",
              "  'end': 12},\n",
              " {'entity_group': 'PERSON',\n",
              "  'score': 0.7050145,\n",
              "  'word': '##awa',\n",
              "  'start': 12,\n",
              "  'end': 15}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\") # pass device=0 if using gpu\n",
        "pipe(\"\"\"Busaba Supasawat        profile a self-starter and quick learner with two-year experience   in financial industry. versatile  skill  sets including problem solving,  quantitative and analytical skills. seeking for career in financial industry where i can utilize and further  develop my  skills  and knowledge to   bring innovative solutions helping company to achieve the company goal.   contact phone: 0838523456  email: kanittha.se@hotmail.com  address: 118/208 condo vtara 36,  soi saen-sa-buy, rama iv rd.,  phra-khanong, klong-toei, bangkok  10110  work experience structured products development - bualuang securities may 2020  present ? develop and conduct structured product pricing and other relevant areas ? analyze data to efficiently expand client-based market ? provide market outlook and trading strategies of structured note to the sales team ? create new tools used to assist the sales team  ? handle project for improving current working flow  derivatives analyst - citibank n.a. april 2019  april 2020 ? review and improve the existing process to increase efficiency by applying automation process ? process settlement and confirmations via verbal and electronics means for derivative and structures product ? handle exceptions and providing constructive solutions to resolve issues  onsite analyst - central group online september 2018  march 2019 ? develop excel spreadsheet to improve day-to-day job ? perform ad hoc analysis to identify business issues and draw recommendation in marketing performance management  ? develop performance tracking on google tag manager on website education tsinghua university, beijing, china [m.sc.] aug 2016  july 2018  ? management science and engineering (mse) program ? department of industrial engineering chulalongkorn university [b. eng.] may 2012  may 2016 ? department of chemical engineering  ? awarded second-class honors computer skills ? vba in excel (advance), python (intermediate), sql (basic) ? excellent working knowledge for tableau, power bi certificate  ic complex 1 language skills  chinese (hsk5)  english (intermediate)   ----------------page (0) break---------------- \"\"\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1xNN34yRmrFHgan8w_XoFQIuDX4AeJdOM",
      "authorship_tag": "ABX9TyNDrhnhGn0YjfiyVJETCoW2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "19d1906fc7624699b980554b35be7464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c020071645bc443abd08af0b0cf2aa5e",
              "IPY_MODEL_375d8d50198142038c4fa3df34003202",
              "IPY_MODEL_8d392f972f0b40ad8ddf481f26728a13"
            ],
            "layout": "IPY_MODEL_4a74b5a02dad42eabb3a31540c2d4899"
          }
        },
        "c020071645bc443abd08af0b0cf2aa5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f39155ca198420087051d19b129dc09",
            "placeholder": "​",
            "style": "IPY_MODEL_5b52eb97a35a45eeaaab0651282788b0",
            "value": "Downloading: 100%"
          }
        },
        "375d8d50198142038c4fa3df34003202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd66afd5136b40129d7944bac181384e",
            "max": 267844284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e4170729df64dde8146e723c28a517d",
            "value": 267844284
          }
        },
        "8d392f972f0b40ad8ddf481f26728a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e7e6c14ca96437f96c9b77f390a65fb",
            "placeholder": "​",
            "style": "IPY_MODEL_8b2bfa3a3f024b8e9a18a77cc6845bff",
            "value": " 268M/268M [00:13&lt;00:00, 14.3MB/s]"
          }
        },
        "4a74b5a02dad42eabb3a31540c2d4899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f39155ca198420087051d19b129dc09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b52eb97a35a45eeaaab0651282788b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd66afd5136b40129d7944bac181384e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e4170729df64dde8146e723c28a517d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e7e6c14ca96437f96c9b77f390a65fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b2bfa3a3f024b8e9a18a77cc6845bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4fa61e098e14cf8a591d9c99c0cc9bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed787a9800fa44a8a8c9e7a70c84c150",
              "IPY_MODEL_3de632eb7f874506a119a4eca07beeab",
              "IPY_MODEL_ccc838124fe64373879d59498d79e551"
            ],
            "layout": "IPY_MODEL_40df6057e3a749c5b8e9fe0a084c098c"
          }
        },
        "ed787a9800fa44a8a8c9e7a70c84c150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_830b5c8168dc40e7887eae020701413b",
            "placeholder": "​",
            "style": "IPY_MODEL_25bed9456b10491592c4e491fd47fee3",
            "value": "100%"
          }
        },
        "3de632eb7f874506a119a4eca07beeab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_862077e0b81e42c9bb047670e9a32f2e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26e52ce991e64b3da949ce7583150997",
            "value": 1
          }
        },
        "ccc838124fe64373879d59498d79e551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daa507a5094d4e45b79bd351d5cf63f6",
            "placeholder": "​",
            "style": "IPY_MODEL_04bcbc2c6f744a049b80e34d8a0d7d52",
            "value": " 1/1 [00:00&lt;00:00,  1.56ba/s]"
          }
        },
        "40df6057e3a749c5b8e9fe0a084c098c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "830b5c8168dc40e7887eae020701413b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25bed9456b10491592c4e491fd47fee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "862077e0b81e42c9bb047670e9a32f2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26e52ce991e64b3da949ce7583150997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "daa507a5094d4e45b79bd351d5cf63f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04bcbc2c6f744a049b80e34d8a0d7d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}