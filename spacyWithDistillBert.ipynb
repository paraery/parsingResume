{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "15oMgvCz79vrWESxhC1tqYjy8nHUMtdQC",
      "authorship_tag": "ABX9TyOl4WimpmIGAFZaDVfoO6wF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paraery/parsingResume/blob/main/spacyWithDistillBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zgF0EFMIbEax"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def get_tokens_with_entities(raw_text: str):\n",
        "    # split the text by spaces only if the space does not occur between square brackets\n",
        "    # we do not want to split \"multi-word\" entity value yet\n",
        "    \n",
        "    raw_tokens = re.split(r\"\\s(?![^\\[]*\\])\", raw_text)\n",
        "\n",
        "    # a regex for matching the annotation according to our notation [entity_value](entity_name)\n",
        "    entity_value_pattern = r\"\\[(?P<value>.+?)\\]\\((?P<entity>.+?)\\)\"\n",
        "    entity_value_pattern_compiled = re.compile(entity_value_pattern, flags=re.I|re.M)\n",
        "\n",
        "    tokens_with_entities = []\n",
        "    lstEnt=[]\n",
        "    txt = raw_text\n",
        "    for raw_token in raw_tokens:\n",
        "        match = entity_value_pattern_compiled.match(raw_token)\n",
        "        if match:\n",
        "            raw_entity_name, raw_entity_value = match.group(\"entity\"), match.group(\"value\")\n",
        "            \n",
        "            # we prefix the name of entity differently\n",
        "            # B- indicates beginning of an entity\n",
        "            # I- indicates the token is not a new entity itself but rather a part of existing one\n",
        "            #txt = raw_text[raw_text.find(str(raw_entity_value))-1 : ]\n",
        "            txt = raw_text.replace(raw_text[int(raw_text.find(raw_entity_value))-1],'',1)\n",
        "            txt = txt.replace('](PERSON)','')\n",
        "            #print(txt)\n",
        "            lstdata = ()\n",
        "            entity_name = f\"{raw_entity_name}\"\n",
        "            entName = createSubEntity(txt.find(raw_entity_value), txt.find(raw_entity_value) + len(raw_entity_value), entity_name)\n",
        "            lstEnt.append(entName)\n",
        "                #tokens_with_entities.append((raw_entity_token, entity_name))\n",
        "    d={\"entities\": lstEnt}\n",
        "            #print(lstEnt)\n",
        "    lstdata=(txt,d)\n",
        "    tokens_with_entities.append(lstdata)\n",
        "    return lstdata\n",
        "def createSubEntity(start,end,tag):\n",
        "    s = (int(start),int(end),tag)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('drive/MyDrive/textfortraining8.csv')\n",
        "lstTxt = df['text'].values.tolist()"
      ],
      "metadata": {
        "id": "Q90OjoG6bPPt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data=[]\n",
        "for a in lstTxt:\n",
        "    train_data.append(get_tokens_with_entities(a))"
      ],
      "metadata": {
        "id": "0SVlamkkbTYz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=[]\n",
        "train=train_data"
      ],
      "metadata": {
        "id": "RTMPfWL4bVnq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('drive/MyDrive/testData.csv')\n",
        "lstTest = df['text'].values.tolist()"
      ],
      "metadata": {
        "id": "Q55wkJDyb5hQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test=[]\n",
        "for a in lstTest:\n",
        "    test.append(get_tokens_with_entities(a))"
      ],
      "metadata": {
        "id": "8lbz2e_AcB7B"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3pMJ6alcD8O",
        "outputId": "68f6ca24-ca5b-4212-c33f-1d3bbe96429e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.7)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "Installing collected packages: spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.4\n",
            "    Uninstalling spacy-3.4.4:\n",
            "      Successfully uninstalled spacy-3.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_32_QqzOcKXj",
        "outputId": "b55d1d92-9d38-4719-fcda-a20f5a5f1687"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy_transformers\n",
            "  Downloading spacy_transformers-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/194.6 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy_transformers) (2.4.6)\n",
            "Collecting transformers<4.27.0,>=3.4.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from spacy_transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy_transformers) (1.22.4)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.5.0 in /usr/local/lib/python3.8/dist-packages (from spacy_transformers) (3.5.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.10.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (6.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.12)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.1.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (4.64.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (8.1.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.0->spacy_transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<4.27.0,>=3.4.0->spacy_transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<4.27.0,>=3.4.0->spacy_transformers) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers<4.27.0,>=3.4.0->spacy_transformers) (6.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.1.2)\n",
            "Installing collected packages: tokenizers, spacy-alignments, huggingface-hub, transformers, spacy_transformers\n",
            "Successfully installed huggingface-hub-0.12.1 spacy-alignments-0.9.0 spacy_transformers-1.2.2 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "bEdmgKMKcRQ7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "_amEtvokca6h"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config /content/drive/MyDrive/Spacy/base_config.cfg /content/drive/MyDrive/Spacy/config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irHQTIgKcfEe",
        "outputId": "f90604d8-749b-4bd6-9095-32169871b493"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-04 11:47:03.344387: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-04 11:47:03.344480: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-04 11:47:03.344497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/content/drive/MyDrive/Spacy/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spacy_doc(file,data):\n",
        "  nlp = spacy.blank(\"en\")\n",
        "  db = DocBin()\n",
        "  for text, annot in tqdm(data):\n",
        "    doc = nlp.make_doc(text)\n",
        "    annot = annot['entities']\n",
        "    ents = []\n",
        "    entity_indices = []\n",
        "    for start, end, label in annot:\n",
        "      skip_entity = False\n",
        "      for idx in range(start,end):\n",
        "        if idx in entity_indices:\n",
        "          skip_entity = True\n",
        "          break\n",
        "      if skip_entity==True:\n",
        "        continue\n",
        "      entity_indices = entity_indices+list(range(start,end))\n",
        "\n",
        "      try:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "      except:\n",
        "        continue\n",
        "      if span is None:\n",
        "        err_data = str([start,end])+\" \"+str(text)+\"\\n\"\n",
        "        file.write(err_data)\n",
        "      else:\n",
        "        ents.append(span)\n",
        "      \n",
        "      doc.ents = ents\n",
        "      db.add(doc)\n",
        "  return db"
      ],
      "metadata": {
        "id": "iBkVfZWvc6-W"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('drive/MyDrive/Spacy/error.txt','w')\n",
        "db = get_spacy_doc(file,train)\n",
        "db.to_disk('train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file,test)\n",
        "db.to_disk('test_data.spacy')\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52YZy2uydCtN",
        "outputId": "73b807aa-cbfc-4741-9cc1-465011dcd867"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 620/620 [00:02<00:00, 289.70it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 164.68it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/drive/MyDrive/Spacy/config.cfg --output ./outputSpacyDistillBert --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QoKNoxQdIrg",
        "outputId": "af3c3739-edc3-4088-a655-8cef77c19acd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-04 11:49:42.823756: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-04 11:49:42.823859: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-04 11:49:42.823878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[38;5;2m✔ Created output directory: outputSpacyDistillBert\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: outputSpacyDistillBert\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-03-04 11:49:51,596] [INFO] Set up nlp object from config\n",
            "[2023-03-04 11:49:51,607] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2023-03-04 11:49:51,611] [INFO] Created vocabulary\n",
            "[2023-03-04 11:49:51,612] [INFO] Finished initializing nlp object\n",
            "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 4.59kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 483/483 [00:00<00:00, 214kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 2.60MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 3.93MB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 268M/268M [00:01<00:00, 203MB/s]\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-03-04 11:50:25,226] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (584 > 512). Running this sequence through the model will result in indexing errors\n",
            "  0       0       15191.83    538.40    0.00    0.00    0.00    0.00\n",
            "  1     200     1584432.05  158202.03    0.00    0.00    0.00    0.00\n",
            "  2     400      212041.88  108770.46    0.00    0.00    0.00    0.00\n",
            "  3     600        3506.77  21677.35    0.00    0.00    0.00    0.00\n",
            "  4     800        3936.72  23157.49    0.00    0.00    0.00    0.00\n",
            "  6    1000        3607.99  21766.78    0.00    0.00    0.00    0.00\n",
            "  7    1200        3634.75  22559.89    0.00    0.00    0.00    0.00\n",
            "  8    1400        3541.36  21584.64    4.46    3.52    6.10    0.04\n",
            "  9    1600        3610.38  22305.15    7.89    6.16   10.98    0.08\n",
            " 10    1800        3451.11  21879.93   17.50   13.29   25.61    0.17\n",
            " 12    2000        3188.12  20480.76   22.66   16.67   35.37    0.23\n",
            " 13    2200        2975.07  21089.79   21.77   16.27   32.93    0.22\n",
            " 14    2400        3097.26  20644.40   30.37   21.81   50.00    0.30\n",
            " 15    2600        2989.44  21339.39   26.72   19.44   42.68    0.27\n",
            " 16    2800        2714.13  19844.37   33.94   24.10   57.32    0.34\n",
            " 18    3000        2674.61  19909.24   33.70   24.08   56.10    0.34\n",
            " 19    3200        2840.15  21096.13   32.35   23.16   53.66    0.32\n",
            " 20    3400        2390.76  18862.69   37.19   26.11   64.63    0.37\n",
            " 21    3600        2822.58  20875.25   35.13   24.87   59.76    0.35\n",
            " 22    3800        2453.86  19539.70   40.14   27.83   71.95    0.40\n",
            " 24    4000        2289.93  19210.55   37.89   26.60   65.85    0.38\n",
            " 25    4200        2539.83  19656.83   37.59   26.50   64.63    0.38\n",
            " 26    4400        2372.75  19823.09   42.07   29.33   74.39    0.42\n",
            " 27    4600        2279.33  19044.51   41.81   29.27   73.17    0.42\n",
            " 28    4800        2279.02  18872.25   43.45   30.29   76.83    0.43\n",
            " 30    5000        2125.75  18701.75   42.91   29.95   75.61    0.43\n",
            " 31    5200        2388.28  19693.62   43.21   30.24   75.61    0.43\n",
            " 32    5400        1880.91  18348.17   43.36   30.39   75.61    0.43\n",
            " 33    5600        2231.23  19368.15   44.44   31.07   78.05    0.44\n",
            " 35    5800        1981.70  18426.45   44.83   31.25   79.27    0.45\n",
            " 36    6000        1915.30  18642.13   44.44   31.07   78.05    0.44\n",
            " 37    6200        2064.29  19018.57   44.98   31.40   79.27    0.45\n",
            " 38    6400        1709.69  17653.87   45.14   31.55   79.27    0.45\n",
            " 39    6600        1881.52  18729.50   45.14   31.55   79.27    0.45\n",
            " 41    6800        2009.63  18277.12   45.67   31.88   80.49    0.46\n",
            " 42    7000        1802.91  18359.82   45.14   31.55   79.27    0.45\n",
            " 43    7200        1838.49  18328.71   45.14   31.55   79.27    0.45\n",
            " 44    7400        1629.21  17972.18   45.67   31.88   80.49    0.46\n",
            " 45    7600        1808.08  18603.24   45.67   31.88   80.49    0.46\n",
            " 47    7800        1558.53  17076.57   45.67   31.88   80.49    0.46\n",
            " 48    8000        1671.83  18578.40   45.14   31.55   79.27    0.45\n",
            " 49    8200        1701.22  17445.94   46.21   32.21   81.71    0.46\n",
            " 50    8400        1525.87  18215.66   45.14   31.55   79.27    0.45\n",
            " 51    8600        1615.58  17571.88   46.21   32.21   81.71    0.46\n",
            " 53    8800        1499.02  17171.46   45.67   31.88   80.49    0.46\n",
            " 54    9000        1570.56  18049.10   45.67   31.88   80.49    0.46\n",
            " 55    9200        1557.73  17930.96   45.67   31.88   80.49    0.46\n",
            " 56    9400        1470.41  17775.41   45.67   31.88   80.49    0.46\n",
            " 57    9600        1474.71  17512.70   46.05   32.06   81.71    0.46\n",
            " 59    9800        1299.62  16989.16   45.67   31.88   80.49    0.46\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "outputSpacyDistillBert/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('outputSpacyDistillBert/model-best')"
      ],
      "metadata": {
        "id": "oNcFgJ-fydR_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('drive/MyDrive/textInResume.csv')\n",
        "dataTest = df['text'].values.tolist()"
      ],
      "metadata": {
        "id": "Uu_aQbVhyqJ9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for d in dataTest:\n",
        "  doc = nlp(d)\n",
        "  print(\"## doc \",i,\" ##\")\n",
        "  for ent in doc.ents:\n",
        "    print(ent.text,\"----\",ent.label_)\n",
        "  i = i+1\n",
        "  print(\"   \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpQlfC4Tyrsl",
        "outputId": "e4190ae3-38c9-49b8-cd20-c67717cbb4d1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## doc  0  ##\n",
            " certification ---- PERSON\n",
            " automation ---- PERSON\n",
            " proficient ---- PERSON\n",
            " erp ---- PERSON\n",
            "supavit attagomol ---- PERSON\n",
            "   \n",
            "## doc  1  ##\n",
            "nisaratwisesbantao bachelor ---- PERSON\n",
            "   \n",
            "## doc  2  ##\n",
            "thirawit jirarungroj ---- PERSON\n",
            "   \n",
            "## doc  3  ##\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " ( ---- PERSON\n",
            " enthusiastic ---- PERSON\n",
            " good ---- PERSON\n",
            " good ---- PERSON\n",
            " self ---- PERSON\n",
            " hard ---- PERSON\n",
            " work ---- PERSON\n",
            " loyalty ---- PERSON\n",
            " ready ---- PERSON\n",
            "   \n",
            "## doc  4  ##\n",
            "thanchanok watcharakitphokin ---- PERSON\n",
            "   \n",
            "## doc  5  ##\n",
            "   \n",
            "## doc  6  ##\n",
            "patcharaya anuntasinkul ---- PERSON\n",
            "   \n",
            "## doc  7  ##\n",
            "jarrukorn pensalaphan ---- PERSON\n",
            "   \n",
            "## doc  8  ##\n",
            "pravittra vimonworachort ---- PERSON\n",
            "   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (584 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## doc  9  ##\n",
            "nattacha kingbualuang ---- PERSON\n",
            "   \n",
            "## doc  10  ##\n",
            "purawat ruangsri ---- PERSON\n",
            "   \n",
            "## doc  11  ##\n",
            "nutratanon mahakhet ---- PERSON\n",
            "   \n",
            "## doc  12  ##\n",
            "kitisak thossaensin ---- PERSON\n",
            "   \n",
            "## doc  13  ##\n",
            "suebtas limsaihua ---- PERSON\n",
            "   \n",
            "## doc  14  ##\n",
            "navara sirijarusvong ---- PERSON\n",
            "   \n",
            "## doc  15  ##\n",
            "chayanit sripradit ---- PERSON\n",
            "   \n",
            "## doc  16  ##\n",
            "pimnares puto ---- PERSON\n",
            "   \n",
            "## doc  17  ##\n",
            "saran thitawiriyayos ---- PERSON\n",
            "   \n",
            "## doc  18  ##\n",
            "pakpoom poodsud ---- PERSON\n",
            "   \n",
            "## doc  19  ##\n",
            "kanittha setthapitayakul ---- PERSON\n",
            "   \n",
            "## doc  20  ##\n",
            "   \n",
            "## doc  21  ##\n",
            "pawith panyasirikul ---- PERSON\n",
            "   \n",
            "## doc  22  ##\n",
            "chotitouch supalanunt ---- PERSON\n",
            "   \n",
            "## doc  23  ##\n",
            "aussadach masun ---- PERSON\n",
            "   \n",
            "## doc  24  ##\n",
            "teeratach jearapaganon ---- PERSON\n",
            "̄© ---- PERSON\n",
            "̄ ̄£ ---- PERSON\n",
            "   \n",
            "## doc  25  ##\n",
            "pratan srikamonpattanawut ---- PERSON\n",
            "   \n",
            "## doc  26  ##\n",
            "natthapat phusrisom ---- PERSON\n",
            "   \n",
            "## doc  27  ##\n",
            "chayanon juntarapartsavorn ---- PERSON\n",
            "   \n",
            "## doc  28  ##\n",
            "narurong saeheng ---- PERSON\n",
            "   \n",
            "## doc  29  ##\n",
            "supachai sumeteenarumit ---- PERSON\n",
            "   \n",
            "## doc  30  ##\n",
            "ittichote sornmeethong ---- PERSON\n",
            "   \n",
            "## doc  31  ##\n",
            "kamolchai suebnipon ---- PERSON\n",
            "   \n",
            "## doc  32  ##\n",
            "   \n",
            "## doc  33  ##\n",
            "nattachai noogure ---- PERSON\n",
            "   \n",
            "## doc  34  ##\n",
            "saksorn pawasakarin ---- PERSON\n",
            "   \n",
            "## doc  35  ##\n",
            "navapuvadol uraikul ---- PERSON\n",
            "   \n",
            "## doc  36  ##\n",
            "̄  ---- PERSON\n",
            "̄© ---- PERSON\n",
            "\u0019; ---- PERSON\n",
            "̄© ---- PERSON\n",
            "̄© ---- PERSON\n",
            "̄; ---- PERSON\n",
            "̄ ̄£;àæêªæà ---- PERSON\n",
            "harit piyapornthana ---- PERSON\n",
            "© ---- PERSON\n",
            "   ---- PERSON\n",
            "̄; ---- PERSON\n",
            "© ---- PERSON\n",
            "̄© ---- PERSON\n",
            "© ---- PERSON\n",
            "̄ ̄|; ---- PERSON\n",
            "̄  ---- PERSON\n",
            "̄ ̄ ---- PERSON\n",
            "   \n",
            "## doc  37  ##\n",
            "setthawuth kangwansakol ---- PERSON\n",
            "   \n",
            "## doc  38  ##\n",
            "angkhahad 78 ---- PERSON\n",
            "   \n",
            "## doc  39  ##\n",
            "veerachaimitmorn creativity ---- PERSON\n",
            "   \n",
            "## doc  40  ##\n",
            "phanchita korsanankittipat ---- PERSON\n",
            "   \n",
            "## doc  41  ##\n",
            "suphakit sangthong ---- PERSON\n",
            "   \n",
            "## doc  42  ##\n",
            "porameht   ---- PERSON\n",
            " design ---- PERSON\n",
            " analyze ---- PERSON\n",
            " investigate ---- PERSON\n",
            " create ---- PERSON\n",
            " breakdown ---- PERSON\n",
            " create ---- PERSON\n",
            " present ---- PERSON\n",
            " data ---- PERSON\n",
            " analysis ---- PERSON\n",
            " analytical ---- PERSON\n",
            " cloud ---- PERSON\n",
            " coding ---- PERSON\n",
            " fluid ---- PERSON\n",
            " perseverant ---- PERSON\n",
            " collaboration ---- PERSON\n",
            " persuasion ---- PERSON\n",
            " explanation ---- PERSON\n",
            " the ---- PERSON\n",
            " the ---- PERSON\n",
            " senior ---- PERSON\n",
            " attended ---- PERSON\n",
            " bronze ---- PERSON\n",
            " good ---- PERSON\n",
            " ms ---- PERSON\n",
            " sketchup ---- PERSON\n",
            " pycharm ---- PERSON\n",
            " speak ---- PERSON\n",
            "   \n",
            "## doc  43  ##\n",
            "jitawat chanpraneet ---- PERSON\n",
            "   \n",
            "## doc  44  ##\n",
            "   \n",
            "## doc  45  ##\n",
            "suwan panjanapongchai ---- PERSON\n",
            "   \n",
            "## doc  46  ##\n",
            "saksorn pawasakarin ---- PERSON\n",
            "   \n",
            "## doc  47  ##\n",
            "kamolwan penpetch ---- PERSON\n",
            "   \n",
            "## doc  48  ##\n",
            "autsadang somboonphol ---- PERSON\n",
            "   \n",
            "## doc  49  ##\n",
            "chanon sattrupinat ---- PERSON\n",
            "   \n",
            "## doc  50  ##\n",
            "   \n",
            "## doc  51  ##\n",
            "   \n",
            "## doc  52  ##\n",
            "phaimathayom rachadabhisek ---- PERSON\n",
            "   \n",
            "## doc  53  ##\n",
            "suchit rojanapatanasombat ---- PERSON\n",
            "   \n",
            "## doc  54  ##\n",
            "   \n",
            "## doc  55  ##\n",
            "tharathep chuayrod ---- PERSON\n",
            "   \n",
            "## doc  56  ##\n",
            "nonthawat aphiwong ---- PERSON\n",
            "   \n",
            "## doc  57  ##\n",
            "tititab srisookco ---- PERSON\n",
            "   \n",
            "## doc  58  ##\n",
            "nuttamol janmanee ---- PERSON\n",
            "   \n",
            "## doc  59  ##\n",
            "apinop soisuwan ---- PERSON\n",
            "   \n",
            "## doc  60  ##\n",
            "kritdanai peerapolchaikul ---- PERSON\n",
            "   \n",
            "## doc  61  ##\n",
            "kittiphong chankong ---- PERSON\n",
            "   \n",
            "## doc  62  ##\n",
            "pakavich veeranarapanich ---- PERSON\n",
            "   \n",
            "## doc  63  ##\n",
            "   \n",
            "## doc  64  ##\n",
            "pisarnwate jitvimol ---- PERSON\n",
            "   \n",
            "## doc  65  ##\n",
            "   \n",
            "## doc  66  ##\n",
            "peerawish tawantarong ---- PERSON\n",
            "   \n",
            "## doc  67  ##\n",
            "   \n",
            "## doc  68  ##\n",
            "komson packdeearporn ---- PERSON\n",
            " corporate ---- PERSON\n",
            " managing ---- PERSON\n",
            " bond ---- PERSON\n",
            " derivatives ---- PERSON\n",
            " var ---- PERSON\n",
            " internet ---- PERSON\n",
            " treasury ---- PERSON\n",
            " office ---- PERSON\n",
            " programming ---- PERSON\n",
            " database ---- PERSON\n",
            " typing ---- PERSON\n",
            " english ---- PERSON\n",
            "   \n",
            "## doc  69  ##\n",
            "ektanat pupat ---- PERSON\n",
            "   \n",
            "## doc  70  ##\n",
            "thapaneesuklapkit logisticsengineer ---- PERSON\n",
            "   \n",
            "## doc  71  ##\n",
            "   \n",
            "## doc  72  ##\n",
            "haruthai jankrajung ---- PERSON\n",
            "   \n",
            "## doc  73  ##\n",
            "apiwut kittiparikun ---- PERSON\n",
            "   \n",
            "## doc  74  ##\n",
            "̄© ---- PERSON\n",
            "̄© ---- PERSON\n",
            "\u0001© ---- PERSON\n",
            "̄; ---- PERSON\n",
            "̄; ---- PERSON\n",
            "\u0015© ---- PERSON\n",
            "   \n",
            "## doc  75  ##\n",
            "arthit thetkham ---- PERSON\n",
            "   \n",
            "## doc  76  ##\n",
            "yonlada nedluecha ---- PERSON\n",
            "   \n",
            "## doc  77  ##\n",
            "   \n",
            "## doc  78  ##\n",
            "tussanakorn rattanaburee ---- PERSON\n",
            "   \n",
            "## doc  79  ##\n",
            "natakit lalitputtichoke ---- PERSON\n",
            "   \n",
            "## doc  80  ##\n",
            "thummarat paklao ---- PERSON\n",
            "   \n",
            "## doc  81  ##\n",
            "patipan mata ---- PERSON\n",
            "   \n",
            "## doc  82  ##\n",
            "tanakrid chanburi ---- PERSON\n",
            "   \n",
            "## doc  83  ##\n",
            "sarakrit thahanthai ---- PERSON\n",
            "   \n",
            "## doc  84  ##\n",
            "naruedom kiatikoon ---- PERSON\n",
            "   \n",
            "## doc  85  ##\n",
            "pattarapon buathong ---- PERSON\n",
            "   \n",
            "## doc  86  ##\n",
            "wittawat hormhuan ---- PERSON\n",
            "   \n"
          ]
        }
      ]
    }
  ]
}